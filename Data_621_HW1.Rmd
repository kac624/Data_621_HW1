---
title: 'Data 621 HW1: MLB Regression Project'
author: "Mohamed Hassan-El Serafi, Chun Shing Leung, Keith Colella, Yina, Qiao, Eddie Xu"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction

In professional sports, attaining the most amount of wins in a season is the ultimate goal. Player and team statistics are commonly used to project and predict the number of wins for an upcoming season. In this analysis, we will use team statistics from every Major League Baseball team from 1871 to 2006 to predict the number of wins for each team. We will address how we handled missing values, created new variables based on the data available to us, and transformed variables to help normalize the data. We will show how we selected our variables for each of the three multiple regression models, and compare the results of each before determining which model to use for our test data. 





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r}
library(tidyverse)
library(gtExtras)
library(vtable)
library(kableExtra)
library(reactable)
library(GGally)
library(corrplot)
library(corrr)
library(caret)
library(leaps)
library(MASS)
library(mice)
library(dlookr)
library(VIM)
library(performance)
library(ggcorrplot)
library(rsample)
library(skimr)
library(performance)
library(car)
library(caret)
library(olsrr)
library(DataExplorer)
```


## Data Exploration





```{r}
mlb_training <- read.csv("https://raw.githubusercontent.com/moham6839/Data_621_HW1/main/moneyball-training-data.csv")
```

```{r}
mlb_test <- read.csv("https://raw.githubusercontent.com/moham6839/Data_621_HW1/main/moneyball-evaluation-data.csv")
```


Since `INDEX` is not a variable we will be using, we dropped it from the training and test sets:

```{r}
mlb_training <- mlb_training %>%
  dplyr::select(-INDEX)
```


```{r}
mlb_test <- mlb_test %>%
  dplyr::select(-INDEX)
```




## Glimpse and Summary of MLB Training Set


```{r}
reactable(mlb_training)
```

```{r}
mlb_training %>%
  glimpse() %>%
  kable() %>%
  kable_styling()
```


```{r}
mlb_training %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

```{r}
skim(mlb_training)
```





### Glimpse and Summary of MLB Test Set


```{r}
reactable(mlb_test)
```






```{r}
mlb_test %>%
  glimpse() %>%
  kable() %>%
  kable_styling()
```



```{r}
mlb_test %>%
  summary() %>%
  kable() %>%
  kable_styling()
```


```{r}
skim(mlb_test)
```









```{r}
mlb_training %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "blue", color="blue") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```







```{r}
ggplot(gather(mlb_training), aes(value)) + 
    geom_histogram(bins = 8) + 
    facet_wrap(~key, scales = 'free_x')
```








```{r}
mlb_training %>% 
  ggplot(aes(TARGET_WINS)) + 
  geom_histogram(bins = 50, fill = 'blue', color="black",) +
  geom_vline(aes(xintercept = mean(TARGET_WINS, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TARGET_WINS, na.rm = T)), col = "yellow", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Wins",
       caption = "* Red line is the mean value and yellow is the median") + 
  theme_classic()
```

The `Wins` column follows a normal distribution. This will be important when deciding which transformation method to use. Since `Wins` will be our dependent variable, using a Box-Cox transformation 





```{r}
mlb_training %>%
  gather(-TARGET_WINS, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = TARGET_WINS)) +
    facet_wrap(~ var, scales = "free") +
    geom_point(fill = "blue", color="blue") +
    geom_smooth(method = "lm", se = FALSE, color = "black") + 
  labs(x = element_blank(), y = "Wins")
```






```{r}
cor_matrix <- mlb_training %>% 
  cor(., use = "complete.obs") 
```




```{r}
ggcorrplot::ggcorrplot(cor_matrix, type = "lower",
          lab = TRUE, lab_size = 2.1, tl.cex = 8)
```






```{r}
cor_matrix[lower.tri(cor_matrix, diag=TRUE)] <- ""
cor_matrix <- cor_matrix %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(Variable, Correlation, -rowname) %>%
  filter(Variable != rowname) %>%
  filter(Correlation != "") %>%
  mutate(Correlation = as.numeric(Correlation)) %>%
  rename(` Variable` = rowname) %>%
  arrange(desc(abs(Correlation))) 
```







```{r}
cor_matrix %>%
  filter(abs(Correlation) > .5) %>%
  kable() %>%
  kable_styling()
```


There were high correlations between batting and pitching stats that were measuring the same feature. For instance, homeruns (HR), Walks (BB), Strikeouts (SO), and Hits (H) for batting and pitching were highly correlated with their respective feature. This will be important to consider when selecting our features for modeling.





```{r}
mlb_training %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(mlb_training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable(caption="<center>Missing Training Data Count and Percentage", align = "c") %>% 
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```








```{r}
mlb_test %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(mlb_test) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent) %>%
  kable(caption="<center>Missing Test Data Count and Percentage", align = "c") %>% 
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```





The amount of missing values in the training and test datasets for `TEAM_BATTING_HBP` is exceptionally large, missing 92% and 93% of its values, respectively. If we imputed the missing values for an amount that large, the results would lack natural variation that could result in an effective model. Combined with their low correlation to `TARGET_WINS`, we decided to drop the column in the training and test data. For the other variables with missing values, we used a K-Nearest Neighbor function from the VIM package to impute the missing values. 


### Flaws of Imputing `TEAM_BATTING_HBP`

Dropping HBP was a difficult decision to make, considering HBP is used to calculate On-Base Percentage and At-Bats (AB), with AB also used to calculate Slugging Percentage and Batting Average. Having additional baseball statistics that are commonly used to evaluate team performance would have been helpful for this analysis. However, as you will see, the additional statistics that were produced when imputing HBP created higher than normal stats, particularly with Batting Average. The highest team batting average ever was accomplished by the Philadelphia Phillies in 1894, which had a Batting Average of .350. The mean of the Batting Average column is 0.5283062. Therefore, we can infer that after imputing HBP, the averages of the other statistics become inflated, and therefore unreliable when predicting the number of wins.


```{r}
mlb_train_imp2 <- mlb_training %>%
  kNN(variable = c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO", "TEAM_BATTING_HBP"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```


```{r}
mlb_train_imp2 <- mlb_train_imp2 %>%
  dplyr::mutate(TEAM_BATTING_1B = TEAM_BATTING_H - dplyr::select(., TEAM_BATTING_2B:TEAM_BATTING_HR) %>% rowSums(na.rm = FALSE)) %>%
  dplyr::mutate(TEAM_BATTING_AB = TEAM_BATTING_H + TEAM_PITCHING_BB + TEAM_BATTING_SO + TEAM_BATTING_HBP) %>%
  dplyr::mutate(TEAM_BATTING_AVERAGE = TEAM_BATTING_H/TEAM_BATTING_AB) %>%
  dplyr::mutate(TEAM_BATTING_OBP = (TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_HBP)/(TEAM_BATTING_AB + TEAM_BATTING_BB + TEAM_BATTING_HBP)) %>%
  dplyr::mutate(TEAM_BATTING_SLG = (TEAM_BATTING_1B + 2*TEAM_BATTING_2B + 3*TEAM_BATTING_3B + 4*TEAM_BATTING_HR)/TEAM_BATTING_AB) %>%
  relocate(TEAM_BATTING_1B, .before = TEAM_BATTING_2B) 
```

```{r}
reactable(mlb_train_imp2)
```


```{r}
mean(mlb_train_imp2$TEAM_BATTING_AVERAGE)
```






### Dropping `TEAM_BATTING_HBP` and Imputing Missing Values




**Training**



```{r}
mlb_training_no_hbp <- mlb_training %>%
  dplyr::select(-TEAM_BATTING_HBP)
```




```{r}
set.seed(123)
mlb_train_imp <- mlb_training_no_hbp %>%
  kNN(variable = c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```


```{r}
reactable(mlb_train_imp)
```


```{r}
sum(is.na(mlb_train_imp))
```



```{r}
set.seed(123)
mlb_train_imp <- mlb_train_imp %>%
  kNN(variable = c("TEAM_BATTING_HR"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```






**Test Set**

```{r}
mlb_test_no_hbp <- mlb_test %>%
  dplyr::select(-TEAM_BATTING_HBP)
```


```{r}
set.seed(123)
mlb_test_imp <- mlb_test_no_hbp %>%
  kNN(variable = c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```


```{r}
reactable(mlb_test_imp)
```


```{r}
sum(is.na(mlb_test_imp))
```






## Data Preparation

When analyzing the dataset, we realized that the number of singles (1B) were not a feature. We can deduce that the total number of homeruns (HR), triples (3B), and doubles (2B) can be subtracted from the total number of hits in order to get the total amount of singles.


```{r}
mlb_train_imp <- mlb_train_imp %>%
  dplyr::mutate(TEAM_BATTING_1B = TEAM_BATTING_H - dplyr::select(., TEAM_BATTING_2B:TEAM_BATTING_HR) %>% rowSums(na.rm = FALSE)) %>%
  relocate(TEAM_BATTING_1B, .before = TEAM_BATTING_2B)
```


```{r}
mlb_train_imp %>% 
  ggplot(aes(TEAM_BATTING_1B)) + 
  geom_histogram(bins = 50, fill = 'blue', color="black",) +
  geom_vline(aes(xintercept = mean(TEAM_BATTING_1B, na.rm = T)), col = "red", lty = 2) +
  geom_vline(aes(xintercept = median(TEAM_BATTING_1B, na.rm = T)), col = "yellow", lty = 2) +
  labs(x = element_blank(),
       y = "Count",
       title = "Distribution of Singles",
       caption = "* Red line is the mean value and yellow is the median") + 
  theme_classic()
```

The number of singles shows a positive right skewness. Let's take a look at the correlation matrix that includes the new feature as well as the imputed data: 

```{r}
cor_matrix2 <- mlb_train_imp %>% 
  cor(., use = "complete.obs") 
```




```{r}
ggcorrplot::ggcorrplot(cor_matrix2, type = "lower",
          lab = TRUE, lab_size = 2.1, tl.cex = 8)
```






### Log-Transforming Variables

Since transforming the variables before imputing missing values helps preserve the relationships between the variables in the regression model, we decided to 




**Training Set**




```{r}
mlb_training_no_hbp <- mlb_training_no_hbp %>%
  dplyr::mutate(TEAM_BATTING_1B = TEAM_BATTING_H - dplyr::select(., TEAM_BATTING_2B:TEAM_BATTING_HR) %>% rowSums(na.rm = FALSE)) %>%
  relocate(TEAM_BATTING_1B, .before = TEAM_BATTING_2B)
```


```{r}
mlb_train_log <- log(mlb_training_no_hbp)
```






```{r}
set.seed(123)
mlb_train_log <- mlb_train_log%>%
  kNN(variable = c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```




```{r}
sum(is.na(mlb_train_log))
```




```{r}
mlb_train_log <- log(mlb_train_log)
```


```{r}
mlb_train_log %>%
  gather(-TARGET_WINS, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = TARGET_WINS)) +
    facet_wrap(~ var, scales = "free") +
    geom_point(fill = "blue", color="blue") +
    geom_smooth(method = "lm", se = FALSE, color = "black") + 
  labs(x = element_blank(), y = "Wins")
```

```{r}
ggplot(gather(mlb_train_log), aes(value)) + 
    geom_histogram(bins = 8) + 
    facet_wrap(~key, scales = 'free_x')
```

```{r}
mlb_train_log %>%
  gather(variable, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "blue", color="blue") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```












**Test Set**

```{r}
mlb_test_no_hbp <- mlb_test_no_hbp %>%
  dplyr::mutate(TEAM_BATTING_1B = TEAM_BATTING_H - dplyr::select(., TEAM_BATTING_2B:TEAM_BATTING_HR) %>% rowSums(na.rm = FALSE)) %>%
  relocate(TEAM_BATTING_1B, .before = TEAM_BATTING_2B)
```




```{r}
mlb_test_log <- log(mlb_test_no_hbp)
```






```{r}
set.seed(123)
mlb_test_log <- mlb_test_log%>%
  kNN(variable = c("TEAM_BASERUN_CS", "TEAM_FIELDING_DP", "TEAM_BASERUN_SB", "TEAM_BATTING_SO", "TEAM_PITCHING_SO"),
      k = 5, numFun = weighted.mean, weightDist = TRUE, imp_var = FALSE)
```





```{r}
sum(is.na(mlb_test_log))
```


```{r}
ggplot(gather(mlb_test_log), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free_x')
```






## Model Building

### Model 1 - Using findCorrelation function from caret package

For our first model, we utilized the findCorrelation function to determine which variables to use in our model:




**Model 1 using Training Set**


```{r}
set.seed(123)
highlyCorDescr <- findCorrelation(cor(mlb_train_imp), cutoff = .50, verbose = TRUE)
```

```{r}
set.seed(123)
keep_these <- names(mlb_train_imp)[!(names(mlb_train_imp) %in% colnames(mlb_train_imp)[highlyCorDescr])]
mlb_train_features <- mlb_train_imp[, keep_these]
```

```{r}
reactable(mlb_train_features)
```




```{r}
set.seed(123)
m1 <- lm(TARGET_WINS ~., data = mlb_train_features)
summary(m1)
```

```{r}
#model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
ols_test_breusch_pagan(m1)
```

```{r}
ols_coll_diag(m1)
```




```{r}
ggplot(m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title="Residual vs. Fitted Values Plot") +
  xlab("Fitted values") +
  ylab("Residuals")
```


```{r}
ggplot(data = m1, aes(x = m1$residuals)) +
    geom_histogram(bins = 10, fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')
```

```{r}
ggplot(data = m1, aes(x = .resid)) +
  geom_histogram(binwidth = 0.4) +
  xlab("Residuals")
```

```{r}
qqnorm(resid(m1))
qqline(resid(m1))
```




```{r}
plot(m1)
```

```{r}
predictions <- predict(m1, newdata = mlb_test_imp)
summary(predictions)
```


```{r}
predictions
```







**Model 1 Using Train Log Set**


```{r}
set.seed(123)
highlyCorDescr2 <- findCorrelation(cor(mlb_train_log), cutoff = .50, verbose = TRUE)
```

```{r}
set.seed(123)
keep_these2 <- names(mlb_train_log)[!(names(mlb_train_log) %in% colnames(mlb_train_log)[highlyCorDescr2])]
mlb_train_features2 <- mlb_train_log[, keep_these2]
```

```{r}
reactable(mlb_train_features2)
```










```{r}
set.seed(123)
m1_log <- lm(TARGET_WINS ~., data = mlb_train_features2)
summary(m1_log)
```

The initial results did not produce a trustworthy model. The Adjusted R-squared value was 21.14%, meaning the independent variables in the model can only account for 21.14% of the variance of `TARGET_WINS`. There were a considerable amount of independent variables that had a p-value greater than 0.05, which indicates that they are not statistically significant and could be affecting the model's ability to determine the impact other variables may have on the explaining the variance of `TARGET_WINS`. We decided to re-run the model without the variables with high p-values to see any improvement in our model. The variables with p-values greater than 0.05 and will be removed are `TEAM_BATTING_2B`, `TEAM_BATTING_3B`, `TEAM_BATTING_HR`, `TEAM_BATTING_SO`, and `TEAM_PITCHING_SO`:


```{r}
set.seed(123)
m1_log2 <- lm(TARGET_WINS ~ TEAM_BATTING_BB + TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_PITCHING_HR + TEAM_PITCHING_H + TEAM_PITCHING_BB + TEAM_FIELDING_DP, data = mlb_train_features2)
summary(m1_log2)
```


Removing the variables did not impact the Adjusted R-squared value, which shows a decrease of 0.02 to 21.12%. 


```{r}
ggplot(m1_log2, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title="Residual vs. Fitted Values Plot") +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r}
ggplot(data = m1_log2, aes(x = .resid)) +
  geom_histogram(binwidth = 0.03) +
  xlab("Residuals")
```


```{r}
qqnorm(resid(m1_log2))
qqline(resid(m1_log2))
```














### Model 2

**Model 2 Using Train Set**


```{r}
filterCtrl <- rfeControl(functions=rfFuncs, method="cv", number=3)
results <- rfe(x= mlb_train_imp[,2:16],y= mlb_train_imp[,1], sizes=c(2:16), rfeControl=filterCtrl)
results
```


```{r}
set.seed(123)
m2 <- lm(TARGET_WINS ~ TEAM_FIELDING_E + TEAM_BATTING_H + TEAM_BASERUN_CS + TEAM_BATTING_BB + TEAM_PITCHING_SO, data = mlb_train_imp)
summary(m2)
```









```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(TARGET_WINS ~., data = mlb_train_imp,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:5),
                    trControl = train.control
                    )
step.model$results
```









**Model 2 Using Train Log Set**


```{r}
mlb_train_log2 <- mlb_train_features2 %>%
  mutate_if(is.numeric, function(x)ifelse(is.infinite(x), median(mlb_train_features2),x))
```


```{r}
filterCtrl2 <- rfeControl(functions=rfFuncs, method="cv", number=3)
results2 <- rfe(x= mlb_train_features2[,2:16],y= mlb_train_features2[,1], sizes=c(2:16), rfeControl=filterCtrl2)
results2
```

```{r}
mlb_train_features
```







```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(TARGET_WINS ~., data = mlb_train_log,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:5),
                    trControl = train.control
                    )
step.model$results
```






### Model 3


```{r}
#use roc_curve area as score
roc_imp <- filterVarImp(x = mlb_train_imp[,2:16], y = mlb_train_imp$TARGET_WINS)

#sort the score in decreasing order
roc_imp <- data.frame(cbind(variable = rownames(roc_imp), score = roc_imp[,1]))
roc_imp$score <- as.double(roc_imp$score)
roc_imp[order(roc_imp$score,decreasing = TRUE),]
```



```{r}
models <- regsubsets(TARGET_WINS ~., data = mlb_train_imp, nvmax = 5,
                     method = "seqrep")
summary(models)
```

```{r}
ols_step_backward_aic(m1)
```




## Model Selection


```{r}

```












## References 


* https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/#:~:text=When%20dealing%20with%20missing%20data,be%20required%2C%20even%20if%20incomplete.

* https://stefvanbuuren.name/fimd/sec-MCAR.html

* https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/#:~:text=Generally%2C%20this%20approach%20is%20not,useful%20data%20from%20the%20dataset.

* https://campus.datacamp.com/courses/handling-missing-data-with-imputations-in-r/donor-based-imputation?ex=9

* https://www.statmuse.com/mlb/ask/which-team-has-the-highest-batting-average-in-a-season

* https://jtr13.github.io/cc21fall2/feature-selection-in-r.html

* http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/#google_vignette

* https://www.bookdown.org/rwnahhas/RMPH/mi-fitting.html
